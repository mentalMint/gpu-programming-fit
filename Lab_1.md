# Лабораторная 1
## Задача
Выделить на GPU массив arr из 10^9 элементов типа float и инициализировать его с помощью ядра следующим образом: arr[i] = sin((i%360)*Pi/180). Скопировать массив в память центрального процессора и посчитать ошибку err = sum_i(abs(sin((i%360)*Pi/180) - arr[i]))/10^9. Провести исследование зависимости результата от использования функций: sin, sinf, __sinf. Объяснить результат. Проверить результат при использовании массива типа double.

## Характеристики устройства GPU
Device Name: Tesla T4 \
Compute Capability: 7.5 \
Total Global Memory (bytes): 15828320256 \
Max Threads per Block: 1024 \
Multiprocessor Count: 40 \
Clock Rate (kHz): 1590000 \
Shared Memory per Block (bytes): 49152 \
Warp Size: 32 \
ECC Enabled: Yes

## Компиляция и запуск
#### *Запуск осуществлялся в Google Colab*
```console
!nvcc -arch=sm_75 --use_fast_math cuda_sin_test.cu -o cuda_sin_test
!./cuda_sin_test
```

## Вывод программы
=== FLOAT === \
Запуск sin() ... \
Время: 420794[µs] \
Ошибка: 0

Запуск sinf() ... \
Время: 16804[µs] \
Ошибка: 1.6e-08

Запуск __sinf() ... \
Время: 16820[µs] \
Ошибка: 1.6e-08


=== DOUBLE === \
Запуск sin() ... \
Время: 309773[µs] \
Ошибка: 8.77963e-18 

Запуск sinf() ... \
Время: 256302[µs] \
Ошибка: 1.30149e-07

Запуск __sinf() ... \
Время: 263361[µs] \
Ошибка: 1.30149e-07

## Результаты запуска
| Type | sin | sinf | __sinf |
| --- | --- | --- | --- |
| Float| 0 | 1.6e-08 | 1.6e-08 |
| Double | 8.77963e-18 | 1.30149e-07 | 1.30149e-07 |

## Выводы
*sin - Double Prectision Mathematical Function \
sinf - Single Precision Mathematical Function \
__sinf - Sinfgle Precision Intrinsic*

1. При вычислениях sin() с типом float, ошибка - 0. sin() - функция с двойной точностью (тип double). При подсчете ошибки используется тип float, поэтому при переводе во float происходит округление и ошибка исчезает. Приведением типов объясняется и долгое время работ программы в этом эксперименте. \
2. При вызове sinf и __sinf с типом double компилятор не может просто использовать быструю аппаратную инструкцию, так как она только для float, поэтому происходит приведение типов, теряется время и точность.
